# SIGE_P2_NN-Classifiers: Terminal Guide

## üìö Overview

This repository was developed for the **SIGE** course. Its goal is to build a **robust bird image classifier**, handling two different resolutions: **20x20** and **200x200**.

The project is modularized into the following components:

- `config.py` ‚Äî Global configurations (paths, training parameters, environment settings).
- `src/data_loader.py` ‚Äî Data loading and preprocessing (transformations, dataset splitting, DataLoaders).
- `src/visualization.py` ‚Äî Visualization tools (examples, pixel distributions, class counts).
- `src/train.py` ‚Äî Model definition, training loop, and validation (fine-tuning a pre-trained model).
- `main.py` ‚Äî Orchestrates the entire workflow (data loading, visualization, training).

---

## ‚öôÔ∏è Installation

1. Create a virtual environment:


```bash
python -m venv venv
```
2. Activate the virtual environment:


**On Windows:**
```bash
.\venv\Scripts\activate
``` 
**On macOS/Linux:**
```bash
source venv/bin/activate
```
Install the required dependencies
```bash
pip install -r requirements.txt
```
‚ö†Ô∏è  Make sure requirements.txt includes packages like:
    - torch
    - torchvision
    - matplotlib
    - etc.

üõ†Ô∏è  Configuration
-----------------
File: config.py

  Dataset directories:
    ‚Ä¢ DATA_DIR_X20   ‚Üí Path to 20x20 image dataset
    ‚Ä¢ DATA_DIR_X200  ‚Üí Path to 200x200 image dataset

  Training parameters:
    ‚Ä¢ BATCH_SIZE
    ‚Ä¢ EPOCHS
    ‚Ä¢ LEARNING_RATE

  MAIN_DATASET environment variable:
    ‚Ä¢ If 'x20'  ‚Üí 200x200 images are downscaled
    ‚Ä¢ If 'x200' ‚Üí 20x20 images are upscaled


üß™ Data Loading & Preprocessing
-------------------------------
Module: src/data_loader.py

  Transformation pipelines (torchvision.transforms):
    ‚Ä¢ Standard Transform     ‚Üí Resize to 224x224
    ‚Ä¢ Augmentation Transform ‚Üí Random crops, flips, rotations
    ‚Ä¢ Scaling Transforms:
        - Downscaling ‚Üí 112x112
        - Upscaling   ‚Üí 448x448

  Dataset loading:
    ‚Ä¢ Using ImageFolder
    ‚Ä¢ Combined with ConcatDataset for augmented versions

  Dataset split:
    ‚Ä¢ 80% Training
    ‚Ä¢ 20% Validation


üìä Visualization
----------------
Module: src/visualization.py

  ‚Ä¢ Display sample image grid (verify augmentations and scaling)
  ‚Ä¢ Plot pixel value distribution histograms
  ‚Ä¢ Print class distribution in dataset


üß† Training
-----------
Module: src/train.py

  ‚Ä¢ Fine-tune pre-trained ResNet18 for bird classification
  ‚Ä¢ Training loop outputs:
      - Training loss
      - Validation accuracy
    (for each epoch)

(venv) PS C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers> python .\main_explicable.py
Mapping de clases principal: {'001.Black_footed_Albatross': 0, '002.Laysan_Albatross': 1, '003.Sooty_Albatross': 2, '004.Groove_billed_Ani': 3, '005.Crested_Auklet': 4, '006.Least_Auklet': 5, '007.Parakeet_Auklet': 6, '008.Rhinoceros_Auklet': 7, '009.Brewer_Blackbird': 8, '010.Red_winged_Blackbird': 9, '011.Rusty_Blackbird': 10, '012.Yellow_headed_Blackbird': 11, '013.Bobolink': 12, '014.Indigo_Bunting': 13, '015.Lazuli_Bunting': 14, '016.Painted_Bunting': 15, '017.Cardinal': 16, '018.Spotted_Catbird': 17, '019.Gray_Catbird': 18, '020.Yellow_breasted_Chat': 19}
Distribuci√≥n de im√°genes en el dataset principal:
Clase 0: 60 im√°genes
Clase 1: 60 im√°genes
Clase 2: 58 im√°genes
Clase 3: 60 im√°genes
Clase 4: 44 im√°genes
Clase 5: 41 im√°genes
Clase 6: 53 im√°genes
Clase 7: 48 im√°genes
Clase 8: 59 im√°genes
Clase 9: 60 im√°genes
Clase 10: 60 im√°genes
Clase 11: 56 im√°genes
Clase 12: 60 im√°genes
Clase 13: 60 im√°genes
Clase 14: 58 im√°genes
Clase 15: 58 im√°genes
Clase 16: 57 im√°genes
Clase 17: 45 im√°genes
Clase 18: 59 im√°genes
Clase 19: 59 im√°genes
Tama√±o del conjunto de entrenamiento combinado: 1784
Conjuntos incluidos en el conjunto de entrenamiento combinado:
        Tama√±o del subconjunto 1: 892
        Tama√±o del subconjunto 2: 892
Tama√±o del conjunto de entrenamiento por clase:
Clase 001.Black_footed_Albatross: 98
Clase 002.Laysan_Albatross: 92
Clase 003.Sooty_Albatross: 98
Clase 004.Groove_billed_Ani: 92
Clase 005.Crested_Auklet: 66
Clase 006.Least_Auklet: 64
Clase 007.Parakeet_Auklet: 78
Clase 008.Rhinoceros_Auklet: 74
Clase 009.Brewer_Blackbird: 92
Clase 010.Red_winged_Blackbird: 96
Clase 011.Rusty_Blackbird: 98
Clase 012.Yellow_headed_Blackbird: 98
Clase 013.Bobolink: 88
Clase 014.Indigo_Bunting: 100
Clase 015.Lazuli_Bunting: 94
Clase 016.Painted_Bunting: 96
Clase 017.Cardinal: 94
Clase 018.Spotted_Catbird: 74
Clase 019.Gray_Catbird: 96
Clase 020.Yellow_breasted_Chat: 96
Traceback (most recent call last):
  File "C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers\main_explicable.py", line 41, in <module>
    main()
  File "C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers\main_explicable.py", line 37, in main
    best_hparams = hyperparameter_tuning(train_dataset, val_dataset, full_dataset, attr_dim=full_dataset.attr_dim)
TypeError: hyperparameter_tuning() got an unexpected keyword argument 'attr_dim'
(venv) PS C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers> python .\main_explicable.py
Mapping de clases principal: {'001.Black_footed_Albatross': 0, '002.Laysan_Albatross': 1, '003.Sooty_Albatross': 2, '004.Groove_billed_Ani': 3, '005.Crested_Auklet': 4, '006.Least_Auklet': 5, '007.Parakeet_Auklet': 6, '008.Rhinoceros_Auklet': 7, '009.Brewer_Blackbird': 8, '010.Red_winged_Blackbird': 9, '011.Rusty_Blackbird': 10, '012.Yellow_headed_Blackbird': 11, '013.Bobolink': 12, '014.Indigo_Bunting': 13, '015.Lazuli_Bunting': 14, '016.Painted_Bunting': 15, '017.Cardinal': 16, '018.Spotted_Catbird': 17, '019.Gray_Catbird': 18, '020.Yellow_breasted_Chat': 19}
Distribuci√≥n de im√°genes en el dataset principal:
Clase 0: 60 im√°genes
Clase 1: 60 im√°genes
Clase 2: 58 im√°genes
Clase 3: 60 im√°genes
Clase 4: 44 im√°genes
Clase 5: 41 im√°genes
Clase 6: 53 im√°genes
Clase 7: 48 im√°genes
Clase 8: 59 im√°genes
Clase 9: 60 im√°genes
Clase 10: 60 im√°genes
Clase 11: 56 im√°genes
Clase 12: 60 im√°genes
Clase 13: 60 im√°genes
Clase 14: 58 im√°genes
Clase 15: 58 im√°genes
Clase 16: 57 im√°genes
Clase 17: 45 im√°genes
Clase 18: 59 im√°genes
Clase 19: 59 im√°genes
Tama√±o del conjunto de entrenamiento combinado: 1784
Conjuntos incluidos en el conjunto de entrenamiento combinado:
        Tama√±o del subconjunto 1: 892
        Tama√±o del subconjunto 2: 892
Tama√±o del conjunto de entrenamiento por clase:
Clase 001.Black_footed_Albatross: 98
Clase 002.Laysan_Albatross: 92
Clase 003.Sooty_Albatross: 98
Clase 004.Groove_billed_Ani: 92
Clase 005.Crested_Auklet: 66
Clase 006.Least_Auklet: 64
Clase 007.Parakeet_Auklet: 78
Clase 008.Rhinoceros_Auklet: 74
Clase 009.Brewer_Blackbird: 92
Clase 010.Red_winged_Blackbird: 96
Clase 011.Rusty_Blackbird: 98
Clase 012.Yellow_headed_Blackbird: 98
Clase 013.Bobolink: 88
Clase 014.Indigo_Bunting: 100
Clase 015.Lazuli_Bunting: 94
Clase 016.Painted_Bunting: 96
Clase 017.Cardinal: 94
Clase 018.Spotted_Catbird: 74
Clase 019.Gray_Catbird: 96
Clase 020.Yellow_breasted_Chat: 96
Traceback (most recent call last):
  File "C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers\main_explicable.py", line 42, in <module>
    main()
  File "C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers\main_explicable.py", line 37, in main
    best_hparams = hyperparameter_tuning(train_dataset, val_dataset, full_dataset)
  File "C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers\src\explicable_train.py", line 110, in hyperparameter_tuning
    num_classes = len(full_dataset.class_to_idx)
AttributeError: 'CUBMultimodalDataset' object has no attribute 'class_to_idx'
(venv) PS C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers> python .\main_explicable.py
Mapping de clases principal: {'001.Black_footed_Albatross': 0, '002.Laysan_Albatross': 1, '003.Sooty_Albatross': 2, '004.Groove_billed_Ani': 3, '005.Crested_Auklet': 4, '006.Least_Auklet': 5, '007.Parakeet_Auklet': 6, '008.Rhinoceros_Auklet': 7, '009.Brewer_Blackbird': 8, '010.Red_winged_Blackbird': 9, '011.Rusty_Blackbird': 10, '012.Yellow_headed_Blackbird': 11, '013.Bobolink': 12, '014.Indigo_Bunting': 13, '015.Lazuli_Bunting': 14, '016.Painted_Bunting': 15, '017.Cardinal': 16, '018.Spotted_Catbird': 17, '019.Gray_Catbird': 18, '020.Yellow_breasted_Chat': 19}
Distribuci√≥n de im√°genes en el dataset principal:
Clase 0: 60 im√°genes
Clase 1: 60 im√°genes
Clase 2: 58 im√°genes
Clase 3: 60 im√°genes
Clase 4: 44 im√°genes
Clase 5: 41 im√°genes
Clase 6: 53 im√°genes
Clase 7: 48 im√°genes
Clase 8: 59 im√°genes
Clase 9: 60 im√°genes
Clase 10: 60 im√°genes
Clase 11: 56 im√°genes
Clase 12: 60 im√°genes
Clase 13: 60 im√°genes
Clase 14: 58 im√°genes
Clase 15: 58 im√°genes
Clase 16: 57 im√°genes
Clase 17: 45 im√°genes
Clase 18: 59 im√°genes
Clase 19: 59 im√°genes
Tama√±o del conjunto de entrenamiento combinado: 1784
Conjuntos incluidos en el conjunto de entrenamiento combinado:
        Tama√±o del subconjunto 1: 892
        Tama√±o del subconjunto 2: 892
Tama√±o del conjunto de entrenamiento por clase:
Clase 001.Black_footed_Albatross: 98
Clase 002.Laysan_Albatross: 92
Clase 003.Sooty_Albatross: 98
Clase 004.Groove_billed_Ani: 92
Clase 005.Crested_Auklet: 66
Clase 006.Least_Auklet: 64
Clase 007.Parakeet_Auklet: 78
Clase 008.Rhinoceros_Auklet: 74
Clase 009.Brewer_Blackbird: 92
Clase 010.Red_winged_Blackbird: 96
Clase 011.Rusty_Blackbird: 98
Clase 012.Yellow_headed_Blackbird: 98
Clase 012.Yellow_headed_Blackbird: 98
Clase 013.Bobolink: 88
Clase 014.Indigo_Bunting: 100
Clase 015.Lazuli_Bunting: 94
Clase 015.Lazuli_Bunting: 94
Clase 016.Painted_Bunting: 96
Clase 017.Cardinal: 94
Clase 018.Spotted_Catbird: 74
Clase 019.Gray_Catbird: 96
Clase 020.Yellow_breasted_Chat: 96
Probando: lr=0.001, batch_size=32, optimizer=adam
C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers\venv\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
C:\Users\adria\OneDrive\Documentos\SIGE_P2_NN-Classifiers\venv\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.      
  warnings.warn(msg)
Epoch 1/10 - Val Acc: 22.42%
Epoch 2/10 - Val Acc: 36.32%
Epoch 3/10 - Val Acc: 29.60%
Epoch 4/10 - Val Acc: 48.43%
Epoch 5/10 - Val Acc: 28.25%
Epoch 6/10 - Val Acc: 68.61%
Epoch 7/10 - Val Acc: 71.30%
Epoch 8/10 - Val Acc: 61.43%
Epoch 9/10 - Val Acc: 60.99%
Epoch 10/10 - Val Acc: 65.02%
Validaci√≥n: 71.30%
Probando: lr=0.001, batch_size=32, optimizer=sgd
Epoch 1/10 - Val Acc: 16.14%
Epoch 2/10 - Val Acc: 34.98%
Epoch 3/10 - Val Acc: 54.26%
Epoch 4/10 - Val Acc: 62.33%
Epoch 5/10 - Val Acc: 72.65%
Epoch 6/10 - Val Acc: 78.03%
Epoch 7/10 - Val Acc: 82.06%
Epoch 8/10 - Val Acc: 82.06%
Epoch 9/10 - Val Acc: 81.61%
Epoch 10/10 - Val Acc: 82.06%
Validaci√≥n: 82.06%
Probando: lr=0.001, batch_size=64, optimizer=adam
Epoch 1/10 - Val Acc: 43.95%
Epoch 2/10 - Val Acc: 66.82%
Epoch 3/10 - Val Acc: 47.53%
Epoch 4/10 - Val Acc: 64.57%
Epoch 5/10 - Val Acc: 62.78%
Epoch 6/10 - Val Acc: 62.78%
Epoch 7/10 - Val Acc: 76.68%
Epoch 8/10 - Val Acc: 69.51%
Epoch 9/10 - Val Acc: 52.02%
Epoch 10/10 - Val Acc: 48.43%
Validaci√≥n: 76.68%
Probando: lr=0.001, batch_size=64, optimizer=sgd
Epoch 1/10 - Val Acc: 8.97%
Epoch 2/10 - Val Acc: 21.52%
Epoch 3/10 - Val Acc: 34.08%
Epoch 4/10 - Val Acc: 47.53%
Epoch 5/10 - Val Acc: 49.33%
Epoch 6/10 - Val Acc: 55.16%
Epoch 7/10 - Val Acc: 60.54%
Epoch 8/10 - Val Acc: 65.02%
Epoch 9/10 - Val Acc: 67.26%
Epoch 10/10 - Val Acc: 71.75%
Validaci√≥n: 71.75%
Probando: lr=0.0001, batch_size=32, optimizer=adam
Epoch 1/10 - Val Acc: 61.88%
Epoch 2/10 - Val Acc: 75.34%
Epoch 3/10 - Val Acc: 81.17%
Epoch 4/10 - Val Acc: 83.41%
Epoch 5/10 - Val Acc: 82.51%
Epoch 6/10 - Val Acc: 85.20%
Epoch 7/10 - Val Acc: 84.75%
Epoch 8/10 - Val Acc: 84.30%
Epoch 9/10 - Val Acc: 83.41%
Epoch 10/10 - Val Acc: 85.20%
Validaci√≥n: 85.20%
Probando: lr=0.0001, batch_size=32, optimizer=sgd
Epoch 1/10 - Val Acc: 8.07%
Epoch 2/10 - Val Acc: 10.31%
Epoch 3/10 - Val Acc: 12.56%
Epoch 4/10 - Val Acc: 14.35%
Epoch 5/10 - Val Acc: 17.49%
Epoch 6/10 - Val Acc: 18.39%
Epoch 7/10 - Val Acc: 22.87%
Epoch 8/10 - Val Acc: 25.11%
Epoch 9/10 - Val Acc: 26.91%
Epoch 10/10 - Val Acc: 28.25%
Validaci√≥n: 28.25%
Probando: lr=0.0001, batch_size=64, optimizer=adam
Epoch 1/10 - Val Acc: 41.26%
Epoch 2/10 - Val Acc: 69.51%
Epoch 3/10 - Val Acc: 79.37%
Epoch 4/10 - Val Acc: 79.82%
Epoch 5/10 - Val Acc: 82.96%
Epoch 6/10 - Val Acc: 84.30%
Epoch 7/10 - Val Acc: 86.10%
Epoch 8/10 - Val Acc: 85.20%
Epoch 9/10 - Val Acc: 86.10%
Epoch 10/10 - Val Acc: 86.55%
Validaci√≥n: 86.55%
Probando: lr=0.0001, batch_size=64, optimizer=sgd
Epoch 1/10 - Val Acc: 4.48%
Epoch 2/10 - Val Acc: 5.38%
Epoch 3/10 - Val Acc: 6.28%
Epoch 4/10 - Val Acc: 6.73%
Epoch 5/10 - Val Acc: 7.17%
Epoch 6/10 - Val Acc: 8.52%
Epoch 7/10 - Val Acc: 9.87%
Epoch 8/10 - Val Acc: 10.76%
Epoch 9/10 - Val Acc: 11.21%
Epoch 10/10 - Val Acc: 12.56%
Validaci√≥n: 12.56%
Mejor configuraci√≥n: {'learning_rate': 0.0001, 'batch_size': 64, 'optimizer': 'adam'} con Val Acc: 86.55%
Epoch 1/10 - Val Acc: 38.57%
Epoch 2/10 - Val Acc: 67.26%
Epoch 3/10 - Val Acc: 78.48%
Epoch 4/10 - Val Acc: 78.03%
Epoch 5/10 - Val Acc: 82.96%
Epoch 6/10 - Val Acc: 83.41%
Epoch 7/10 - Val Acc: 82.51%
Epoch 8/10 - Val Acc: 82.96%
Epoch 9/10 - Val Acc: 83.86%
Epoch 10/10 - Val Acc: 84.75%
Hiperpar√°metros √≥ptimos: {'learning_rate': 0.0001, 'batch_size': 64, 'optimizer': 'adam'}



Dise√±o de un Modelo de Clasificaci√≥n Multimodal Explicable para CUB-200-2011
1. Introducci√≥n
El conjunto de datos CUB-200-2011 (Caltech-UCSD Birds-200-2011) es un benchmark de clasificaci√≥n fine-grained de especies de aves. Contiene 200 categor√≠as de p√°jaros con un total de 11.788 im√°genes, junto con anotaciones detalladas como 312 atributos binarios por imagen (por ejemplo, colores de partes del ave)
paperswithcode.com
. Adem√°s, incluye ubicaciones de partes (picos, alas, etc.) y cuadros delimitadores, aunque en este caso nos centramos en los atributos. El problema que abordamos es dise√±ar un enfoque multimodal que combine la informaci√≥n visual de las im√°genes con las caracter√≠sticas adicionales (atributos) proporcionadas en un archivo de texto (formato "etiqueta : nombre_atributo: valor" como '... : has_wing_color: blue' o '... : has_primary_shape: 23,24'). El objetivo es lograr una clasificaci√≥n m√°s precisa de la especie de ave y, a la vez, que el modelo pueda justificar sus decisiones de forma comprensible para humanos. La clasificaci√≥n fine-grained de aves es un reto debido a las sutiles diferencias visuales entre especies (p. ej., variaciones de color o forma en las alas o el pico). Incluir atributos etiquetados manualmente (como color de alas = azul) puede mejorar el reconocimiento, actuando como pistas de alto nivel. Un enfoque que integre ambos tipos de datos promete no solo mayor exactitud sino tambi√©n mejor interpretabilidad, pues las predicciones pueden explicarse en t√©rminos de caracter√≠sticas sem√°nticas (ej. "alas azules") adem√°s de evidencias visuales. Investigaciones previas han demostrado que modelos que utilizan conceptos/atributos humanos alcanzan desempe√±os competitivos con redes neuronales end-to-end, al tiempo que permiten interpretaciones en t√©rminos de dichos atributos
github.com
. En este informe se propone una arquitectura multimodal que combina una CNN para im√°genes con una red densa para los atributos, detallando c√≥mo preprocesar y vectorizar estos atributos. Tambi√©n se discuten diversas t√©cnicas de explicabilidad para entender las predicciones del modelo: desde mapas de calor visuales (Grad-CAM, LIME) hasta explicaciones basadas en atributos o reglas simb√≥licas. Por √∫ltimo, se mencionan herramientas pr√°cticas para implementar la soluci√≥n (PyTorch, TensorFlow, scikit-learn, etc.) y criterios para evaluar tanto el rendimiento predictivo como la calidad de las explicaciones generadas.
2. Arquitectura Multimodal Propuesta


Figura 1: Esquema de la arquitectura propuesta. La arquitectura integrada consta de dos ramas principales que procesan cada modalidad por separado, fusion√°ndose antes de la predicci√≥n final. En la primera rama, una red neuronal convolucional (CNN) procesa la imagen de entrada y extrae un vector de features visuales de alto nivel. En paralelo, la segunda rama procesa los atributos textuales asociados (caracter√≠sticas estructuradas) convirti√©ndolos en un vector num√©rico. Estas dos representaciones se concatenan para formar una representaci√≥n conjunta, que luego alimenta a capas completamente conectadas (fully connected) que realizan la clasificaci√≥n de la especie de ave. En t√©rminos concretos, la rama visual puede ser una CNN pre-entrenada (por ejemplo, ResNet, VGG o EfficientNet) truncada antes de la capa de clasificaci√≥n, de modo que su salida sea un embebido visual de dimensi√≥n fija (p. ej. 2048 dimensiones en ResNet50). La rama de atributos puede consistir en una peque√±a red feed-forward: por ejemplo, una o dos capas densas que toman como entrada el vector de atributos preprocesados (ver secci√≥n de Preprocesamiento) y producen un embebido de atributos de, digamos, 256 dimensiones. La fusi√≥n multimodal se realiza t√≠picamente en la etapa intermedia o final: la opci√≥n m√°s sencilla es concatenar el vector de caracter√≠sticas visuales y el vector de atributos, y luego pasar este vector combinado por una o m√°s capas densas que emitan las 200 clases mediante softmax. Este enfoque de combinar CNN con datos tabulares es una pr√°ctica com√∫n y efectiva
stackoverflow.com
: se aprovecha la capacidad de la CNN para aprender representaciones visuales y del MLP (perceptr√≥n multicapa) para los atributos, uni√©ndolos en un modelo √∫nico. Alternativas de fusi√≥n: Una variante posible es la fusi√≥n temprana, donde los atributos influyen en etapas intermedias de la CNN (por ejemplo, concatenando el vector de atributos con las activaciones de alguna capa oculta de la CNN en lugar de solo al final). Sin embargo, una fusi√≥n tard√≠a (post-extracci√≥n de caracter√≠sticas) suele ser m√°s sencilla de entrenar y aprovechar las modalidades por separado. Otra alternativa interesante es utilizar un enfoque de bottleneck de conceptos: primero entrenar un modelo para predecir los atributos a partir de la imagen, y luego otro modelo (o capa) que prediga la especie a partir de esos atributos predichos
github.com
. Este enfoque, conocido como Concept Bottleneck Model (CBM), hace que la predicci√≥n pase por la capa de atributos como representaci√≥n intermedia interpretable. En la pr√°ctica se puede implementar de manera conjunta: la CNN primero produce predicciones de los 312 atributos (con supervisi√≥n de los atributos reales en entrenamiento), luego esas predicciones se usan como entrada a la capa de clasificaci√≥n final. Esto garantiza que las decisiones se basen expl√≠citamente en los atributos, facilitando explicaciones (ver secci√≥n Explicabilidad). No obstante, los CBM pueden sacrificar algo de precisi√≥n si los atributos predichos no son perfectos. Por ello, una opci√≥n h√≠brida es entrenar el modelo multimodal end-to-end con ambas p√©rdidas: p√©rdida de clasificaci√≥n de especie y, opcionalmente, p√©rdida de reconstrucci√≥n de atributos, para alentar al modelo a prestar atenci√≥n a esas caracter√≠sticas. En resumen, la arquitectura propuesta integra eficientemente las dos fuentes de datos: la imagen proporciona informaci√≥n visual detallada y los atributos aportan conocimiento estructurado adicional. Esta combinaci√≥n multimodal deber√≠a mejorar la precisi√≥n al resolver ambig√ºedades visuales con pistas textuales, a la vez que establece la base para explicaciones m√°s transparentes, ya que el modelo maneja conceptos entendibles (los atributos) internamente.
3. Preprocesamiento y Vectorizaci√≥n de Atributos
Los atributos suministrados en el dataset vienen en formato texto con la sintaxis "etiqueta: nombre_atributo: valor". Antes de alimentar estos datos al modelo, es necesario transformarlos en un vector num√©rico manejable. Suponiendo que cada imagen (o cada instancia) tiene una serie de l√≠neas describiendo propiedades, el primer paso es parsear el archivo de atributos. Por ejemplo, consideremos las entradas:
12 : has_wing_color: blue
12 : has_primary_shape: 23,24
Estas l√≠neas podr√≠an indicar que para la imagen con etiqueta 12 (especie particular), el color de las alas es azul y tiene cierta forma primaria identificada por √≠ndices 23 y 24. En el CUB, los atributos suelen ser binarios predefinidos (312 atributos posibles, e.g. "tiene alas azules = s√≠/no")
paperswithcode.com
. En la pr√°ctica, podemos construir un vocabulario de atributos a partir de todos los pares nombre_atributo=valor presentes en el conjunto de datos. Cada combinaci√≥n √∫nica se convertir√° en una dimensi√≥n en el vector de atributos. En este caso, podr√≠amos descomponer has_wing_color: blue en un atributo binario "wing_color_blue" que vale 1 para esta ave. De forma similar, si has_wing_color: red aparece en otra instancia, tendr√≠amos "wing_color_red" como otra caracter√≠stica. Para atributos num√©ricos o con m√∫ltiples valores (como has_primary_shape: 23,24), interpretaremos que la propiedad primary_shape incluye las categor√≠as 23 y 24. Esto se puede representar activando dos caracter√≠sticas binarias: por ejemplo "primary_shape_23" = 1 y "primary_shape_24" = 1 para esa imagen. En general, cada atributo puede procesarse as√≠:
Atributos categ√≥ricos (texto): convertir a one-hot encoding. Cada posible valor de ese atributo se vuelve un componente binario. Ej: atributo "wing_color" con posibles valores {blue, brown, black,...} genera varias columnas (wing_color_blue, wing_color_brown, etc.), colocando 1 en la correspondiente al valor presente y 0 en las dem√°s.
Atributos multi-valor: si un atributo puede tener lista de valores (como m√∫ltiples partes presentes), se activan todas las columnas correspondientes. (Alternativamente, se podr√≠a asignar m√∫ltiples valores a un embedding, pero dado que son pocos y fijos es m√°s simple multi-hot).
Atributos num√©ricos continuos: si existieran (ej: longitud de ala = 5.2 cm), se pueden normalizar o discretizar seg√∫n convenga. (En CUB, pr√°cticamente todos los atributos son discretos/categ√≥ricos o booleanos).
Tras este proceso, cada imagen queda representada por un vector de atributos de dimensi√≥n fija (la cantidad total de atributos √∫nicos en el dataset). Con CUB, ese tama√±o ser√° del orden de 312 (si usamos directamente los 312 atributos binarios originales) hasta algunos cientos m√°s si diferenciamos valores distintos de un mismo atributo como features separadas. Dado que el dataset ya define 312 atributos binarios, una buena pr√°ctica es utilizar esa estructura: por ejemplo, los atributos podr√≠an incluir cosas como wing color = blue, wing pattern = striped, bill shape = curved, etc., cada uno binario indicando presencia/ausencia de esa caracter√≠stica en la imagen. Podemos mapear el texto a esos √≠ndices conocidos. Si el archivo no da directamente el vector de 312 bits por imagen, tendremos que generarlo: inicializar un vector de 312 ceros y poner 1 en las posiciones correspondientes a los atributos mencionados en las l√≠neas para esa imagen. Una vez vectorizados los atributos, conviene escalarlos o normalizarlos si son continuos; en caso de atributos binarios o one-hot, no se requiere escalado especial (0/1 ya est√° bien). Tambi√©n es √∫til dividir los datos en entrenamiento/validaci√≥n/test asegurando que para cada imagen la fila de atributos se alinea con la imagen en el modelo (por ejemplo, usando el mismo ID o nombre de archivo para hacer join entre la tabla de atributos y las im√°genes). Durante el entrenamiento, podremos alimentar al modelo pares (imagen, vector_atributos) como entrada conjunta, mientras que en inferencia se requerir√≠a disponer de los atributos para cada nueva imagen. Si en una aplicaci√≥n real no se tuvieran los atributos manualmente, podr√≠amos primero predecirlos con un sub-modelo, como se coment√≥ en el esquema de concept bottleneck. En resumen, el preprocesamiento transforma las descripciones como "has_wing_color: blue" en variables num√©ricas utilizables por la red. Esto permite que la informaci√≥n sem√°ntica de los atributos se integre con las caracter√≠sticas visuales aprendidas por la CNN, aportando contexto adicional para diferenciar especies parecidas (por ejemplo, dos aves visualmente similares podr√≠an distinguirse porque una tiene patas rojas y otra patas amarillas, informaci√≥n que el vector de atributos explicitamente codifica).
4. Explicabilidad del Modelo
Una vez entrenado el modelo multimodal, es crucial disponer de mecanismos que expliquen por qu√© se tom√≥ una decisi√≥n determinada. La explicabilidad se puede abordar desde dos frentes complementarios: (1) explicaciones visuales que se√±alan qu√© partes de la imagen influyeron en la clasificaci√≥n, y (2) explicaciones simb√≥licas o basadas en atributos que describen la predicci√≥n en t√©rminos de las caracter√≠sticas de alto nivel (color, forma, etc.). A continuaci√≥n, describimos varias t√©cnicas en ambas categor√≠as y otras adicionales, junto con recomendaciones para su uso.
4.1 Explicaciones Visuales (saliency maps)
Grad-CAM (Gradient-weighted Class Activation Mapping) ‚Äì Es una t√©cnica popular para obtener una visualizaci√≥n de las √°reas de la imagen a las que la CNN prest√≥ m√°s atenci√≥n para predecir una cierta clase. Grad-CAM calcula la importancia de cada regi√≥n a partir de los gradientes de la clase objetivo con respecto a las activaciones de las √∫ltimas capas convolucionales. El resultado es un mapa de calor sobre la imagen original, donde las zonas m√°s calientes (rojo/intenso) indican mayor contribuci√≥n a la predicci√≥n de esa clase
adataodyssey.com
. En esencia, Grad-CAM destaca cu√°les partes de la imagen fueron usadas por el modelo para clasificar la imagen como una especie particular. Por ejemplo, si el modelo identifica correctamente un Blue Jay, Grad-CAM podr√≠a resaltar la regi√≥n de las alas o el pecho azul del ave, mostrando que esos p√≠xeles influyeron fuertemente en la decisi√≥n. Este m√©todo es independiente del modelo en el sentido de que funciona con cualquier CNN con arquitectura con capas convolucionales finales (no requiere modificar el entrenamiento). Es r√°pido de calcular y provee una explicaci√≥n visual intuitiva ‚Äì para un humano es f√°cil verificar si el modelo mir√≥ al p√°jaro (y qu√© parte del p√°jaro) o si se distrajo con el fondo. LIME (Local Interpretable Model-Agnostic Explanations) ‚Äì Mientras Grad-CAM es espec√≠fico de CNNs, LIME es un m√©todo agn√≥stico al modelo que puede aplicarse tambi√©n a im√°genes. En el caso de im√°genes, LIME genera explicaciones locales entrenando un modelo interpretable (por ejemplo, un modelo lineal con pocas variables) en la vecindad de la instancia a explicar
vishnudsharma.medium.com
proceedings.mlr.press
. ¬øC√≥mo se logra esto? Primero, LIME segmenta la imagen en regiones significativas llamadas superp√≠xeles (grupos de p√≠xeles contiguos con color/textura similar)
vishnudsharma.medium.com
. Cada superp√≠xel se trata como una caracter√≠stica interpretable que puede estar "presente" u "oculta". Luego LIME genera muchas versiones de la imagen perturbadas al azar, encendiendo y apagando superp√≠xeles (p. ej., quitando o griseando ciertas regiones)
proceedings.mlr.press
. Para cada versi√≥n perturbada obtiene la predicci√≥n del modelo original (la probabilidad de cada clase). Con ese conjunto de datos (presencia/ausencia de regiones -> resultado del modelo), LIME ajusta un modelo lineal aproximado que predice la output del modelo en funci√≥n de qu√© superp√≠xeles est√°n presentes. Las ponderaciones de ese modelo lineal sirven como explicaci√≥n: indican cu√°les segmentos de la imagen contribuyen positiva o negativamente a la clasificaci√≥n actual. LIME normalmente resaltar√°, por ejemplo, 3-5 regiones de la imagen: marcando en verde las que m√°s aportan a favor de la predicci√≥n y en rojo las que van en contra. En el contexto de CUB, LIME podr√≠a, por ejemplo, se√±alar que la regi√≥n correspondiente a las alas y cola (de color azul) aport√≥ positivamente a predecir Blue Jay, mientras que quiz√° la regi√≥n del fondo cielo neutro no fue importante. LIME es √∫til porque no depende de acceder a gradientes internos, funciona con cualquier modelo como caja negra, y permite tambi√©n explicar fallos del modelo al ver qu√© partes lo confundieron. Como desventaja, es computacionalmente m√°s costoso (muchas evaluaciones del modelo con perturbaciones) y tiene cierta variabilidad en cada ejecuci√≥n local. Otras t√©cnicas visuales ‚Äì Existen otros m√©todos de saliency o mapas de importancia a nivel de p√≠xel similares a Grad-CAM y LIME. Por ejemplo, Integrated Gradients calcula la contribuci√≥n de cada p√≠xel integrando los gradientes mientras se var√≠a la imagen desde un estado de referencia (como una imagen negra) hasta la actual. El resultado es otro mapa de relevancia de pixels, m√°s fino que Grad-CAM (que opera a nivel de caracter√≠stica de alto nivel) y que satisface propiedades te√≥ricas de aditividad. Tambi√©n se puede emplear Occlusion analysis, que consiste en ocluir (enmascarar) sistem√°ticamente diferentes partes de la imagen y medir cu√°nto baja la confianza en la predicci√≥n; las regiones cuya ausencia m√°s reduce la puntuaci√≥n de la clase son las m√°s importantes. M√©todos basados en attention (si la arquitectura tuviera mecanismos de atenci√≥n) tambi√©n permiten visualizar a qu√© pixeles o regiones atendi√≥ el modelo. En general, la recomendaci√≥n es utilizar Grad-CAM como primera opci√≥n por su facilidad y rapidez, complement√°ndolo con LIME o Integrated Gradients para validaci√≥n. Por ejemplo, si Grad-CAM y LIME ambos se√±alan las alas azules, tenemos alta confianza de que esa caracter√≠stica visual es clave para la predicci√≥n.
4.2 Explicaciones basadas en Atributos y Reglas Simb√≥licas
Dado que nuestro modelo incorpora atributos expl√≠citamente, podemos explotar esto para obtener explicaciones simb√≥licas m√°s cercanas al razonamiento humano. La idea central es expresar la decisi√≥n del modelo en t√©rminos de los atributos de alto nivel (color, forma, etc.) que describen al ave. Una forma sencilla de hacerlo es inspeccionando la contribuci√≥n de cada atributo en el modelo. Si la rama de atributos en la red termina en una capa totalmente conectada hacia las clases, los pesos de esa capa indican cu√°nto influencia tiene cada atributo en cada clase. Por ejemplo, supongamos que para la clase Blue Jay el peso asociado al atributo wing_color_blue es muy alto; eso sugiere que tener alas azules es un fuerte indicador de esa especie (lo cual intuitivamente es cierto). De modo similar, si el atributo has_primary_color_red tiene peso negativo para Blue Jay pero positivo para Northern Cardinal, refleja que el modelo entiende que el cardinal es rojo mientras el Blue Jay no. Estas relaciones peso-clase pueden traducirse en explicaciones del tipo: "El modelo predijo Blue Jay en gran parte porque detect√≥ los atributos wing_color=blue y has_black_stripes=yes, los cuales son caracter√≠sticos de esa especie." Durante la inferencia en una instancia concreta, se pueden listar los atributos m√°s influyentes presentes en esa imagen. Muchos de estos atributos el modelo los recibe ya (si usamos los verdaderos), pero si se estuvieran prediciendo internamente (concept bottleneck), igual podemos tomar los valores predichos. Otra t√©cnica formal es entrenar un modelo interpretable surrogate usando los atributos. Por ejemplo, se puede ajustar un √°rbol de decisi√≥n o un conjunto de reglas l√≥gico sobre los vectores de atributos para aproximar las predicciones del modelo complejo. Los √°rboles de decisi√≥n producen reglas if-then f√°cilmente entendibles (ejemplo de interpretabilidad global: "si wing_color = blue y bill_shape = short entonces especie = Blue Jay"), que ayudan a comprender qu√© combinaciones de caracter√≠sticas llevan a ciertas clasificaciones. Podemos usar los datos de entrenamiento (atributos -> predicci√≥n modelo) para inducir dicho √°rbol. Si el √°rbol logra alta fidelidad, significar√≠a que el modelo b√°sicamente tom√≥ decisiones similares a esas reglas. Este m√©todo es similar a LIME pero a nivel global y solo sobre los atributos, y proporciona una explicaci√≥n simb√≥lica compacta. Alternativamente, podr√≠amos extraer reglas por clase inspeccionando qu√© atributos son casi siempre verdaderos para las im√°genes clasificadas como esa clase y ausentes en otras (m√©todos de inducci√≥n de reglas o asociaci√≥n). Por ejemplo, podemos descubrir reglas como "si wing_pattern = spotted y eye_color = red, entonces Speckled Hawk" si resulta consistente en los datos. Una ventaja de disponer de atributos expl√≠citos es que las explicaciones pueden darse en lenguaje natural estructurado. Es factible crear plantillas del estilo: "Esta ave fue clasificada como [Predicci√≥n] porque tiene [atributo1], [atributo2] y [atributo3], lo cual coincide con las caracter√≠sticas distintivas de esa especie." Por ejemplo: "Predicci√≥n: American Goldfinch. Raz√≥n: el modelo detect√≥ plumaje amarillo brillante y alas negras con marcas blancas, atributos caracter√≠sticos de un jilguero americano." Estas explicaciones combinan lo visual con lo sem√°ntico: los atributos detectados pueden provenir de la imagen pero se presentan en palabras comprensibles. Si se dispone de los atributos originales de la imagen, b√°sicamente se est√° reusando la etiqueta humana en la explicaci√≥n, lo cual garantiza interpretabilidad (aunque en cierto sentido trivial si el humano ya lo anot√≥). Pero incluso si los atributos son predichos autom√°ticamente, presentarlos al usuario valida que el modelo "vio" las mismas propiedades que un experto usar√≠a para identificar la especie. En el contexto de nuestro modelo, para dar explicaciones simb√≥licas por instancia se puede proceder as√≠: tomar el vector de atributos de la imagen (ya sea el real o el que predice internamente el modelo), y destacar aquellos que m√°s aumentan la probabilidad de la clase predicha. Por ejemplo, usando t√©cnicas tipo SHAP o importancias de permutation (ver siguiente secci√≥n), podemos calcular la contribuci√≥n de cada atributo en la predicci√≥n espec√≠fica. Luego formamos una frase con los 2-3 atributos positivos principales. Adicionalmente, podr√≠amos mencionar alg√∫n atributo ausente que habr√≠a sugerido otra especie (p. ej., "carece de parches rojos en la cabeza, descartando el Woodpecker"). Esto da una explicaci√≥n contrastiva ligera. Por √∫ltimo, si implementamos el modelo como un Concept Bottleneck, la explicaci√≥n es intr√≠nseca: el modelo primero estima los conceptos (atributos) y luego la clase, por lo que simplemente reportando qu√© conceptos fueron activados obtenemos la justificaci√≥n. Estudios han mostrado que estos modelos permiten incluso intervenci√≥n humana: si el modelo se equivoca en la predicci√≥n, a veces corregir manualmente un atributo predicho mal puede arreglar la clasificaci√≥n
github.com
, lo que refuerza la idea de que las decisiones est√°n efectivamente basadas en esos atributos claros.
4.3 Otros m√©todos y herramientas de explicabilidad
Adem√°s de Grad-CAM, LIME y las reglas basadas en atributos, existen otros enfoques √∫tiles para entender y validar las decisiones del modelo:
SHAP (SHapley Additive exPlanations) ‚Äì SHAP asigna a cada caracter√≠stica un valor de Shapley, basado en teor√≠a de juegos, que representa su contribuci√≥n a la diferencia entre la predicci√≥n del modelo y la media base
datacamp.com
. Aplicado a nuestro modelo, podr√≠amos utilizar SHAP para obtener importancias tanto de p√≠xeles/regiones (agrup√°ndolos quiz√° en superp√≠xeles) como de atributos tabulares. En particular, SHAP es muy pr√°ctico para los atributos: nos dar√≠a una puntuaci√≥n para cada atributo indicando cu√°nto elev√≥ o disminuy√≥ la probabilidad de cada clase en esa instancia. Por ejemplo, SHAP podr√≠a cuantificar que wing_color_blue aport√≥ +0.20 a la probabilidad de Blue Jay, mientras que wing_color_blue aportar√≠a -0.15 a la probabilidad de Cardinal (porque contradice el rojo del cardinal). Estas explicaciones son similares a las obtenidas por LIME pero con s√≥lidas garant√≠as te√≥ricas de equidad en el reparto de importancia
datacamp.com
. Existen implementaciones (como la librer√≠a SHAP en Python) que soportan modelos de deep learning y tabulares, proporcionando visualizaciones como gr√°ficos de barras de importancia o gr√°ficos de dependencia. SHAP nos ayudar√≠a a validar globalmente qu√© atributos son m√°s utilizados por el modelo (promediando valores absolutos) y localmente entender cada predicci√≥n.
Supervisi√≥n de Atenci√≥n/Partes ‚Äì Si en lugar de (o adicional a) atributos, el modelo usara anotaciones de partes (por ej. localizar la cabeza, alas, cola), se podr√≠a incorporar un m√≥dulo de atenci√≥n o detecci√≥n de partes. Esto har√≠a el modelo m√°s interpretable porque podr√≠amos ver d√≥nde mir√≥ para cada parte. Por ejemplo, algunos modelos fine-grained detectan primero partes del ave (cabeza, alas) y luego clasifican; esas partes detectadas sirven como explicaciones: "identific√≥ que el ave tiene la cabeza roja y el ala con barras negras". En nuestro caso, podr√≠amos entrenar sub-modelos auxiliares para predecir la presencia de cada atributo localizando la regi√≥n correspondiente. Herramientas como Grad-CAM tambi√©n pueden aplicarse a salidas intermedias (por ejemplo, al neuron de "blue wing" en la red de atributos) para ver si la CNN realmente mira a las alas cuando activa ese atributo ‚Äì esto combina explicabilidad visual y simb√≥lica.
Ejemplos protot√≠picos ‚Äì Otra t√©cnica explicativa es proporcionar ejemplos similares o prototipos. Dado que tenemos un conjunto de entrenamiento etiquetado, podemos al predecir una imagen recuperar las im√°genes de entrenamiento m√°s cercanas en el espacio de caracter√≠sticas conjunto. Si, por ejemplo, el modelo clasifica una imagen como Western Meadowlark, podr√≠amos mostrar al usuario otras im√°genes de Western Meadowlark del set que el modelo considere parecidas (quiz√° porque tambi√©n tienen pecho amarillo y una banda negra en el cuello). Esto ayuda a contextualizar la predicci√≥n: "se predijo X porque la imagen se parece a estos ejemplos conocidos de X". Incluso se pueden mostrar comparaciones con la especie m√°s probable alternativa, mostrando diferencias. Esta t√©cnica no explica atributos espec√≠ficos, pero proporciona una intuici√≥n basada en casos.
Contra-factuales ‚Äì Generar explicaciones contrafactuales consiste en responder: "¬øqu√© tendr√≠a que cambiar en esta entrada para que el modelo diera otra clasificaci√≥n?". Con atributos, esto es relativamente f√°cil: por ejemplo, "si esta ave no tuviera las alas azules (supongamos fueran marrones), el modelo la clasificar√≠a como un Sparrow en lugar de Blue Jay*". Esa es una explicaci√≥n contrafactual que se√±ala la influencia de "alas azules". Podemos automatizar esto probando a modificar atributos clave y viendo c√≥mo cambia la predicci√≥n. En im√°genes puras es m√°s dif√≠cil (implica editar la imagen), pero con atributos es factible y muy interpretable ("cambia X a Y, modelo output cambia a ..."). Esto puede dar confianza de que el modelo captura relaciones coherentes.
Resumen de m√©todos de explicabilidad: A modo de s√≠ntesis, la siguiente tabla resume las opciones mencionadas, indicando el tipo de explicaci√≥n que brindan:
M√©todo	Descripci√≥n breve	Modalidad principal
Grad-CAM	Mapa de calor sobre la imagen que resalta las regiones relevantes utilizadas por la CNN para una clase dada (p. ej., se√±ala partes del ave importantes para la predicci√≥n).	Visual (imagen)
LIME (im√°genes)	Modelo lineal local sobre superp√≠xeles de la imagen; indica qu√© segmentos de la imagen contribuyen positiva o negativamente a la predicci√≥n actual.	Visual (imagen)
√Årbol de Decisi√≥n (Reglas)	Aproxima el modelo usando un √°rbol sobre los atributos; produce reglas tipo if-then comprensibles (ej: si ala=azul y pico=corto entonces Blue Jay), revelando la l√≥gica basada en atributos.	Simb√≥lico (atributos)
SHAP (Valores Shapley)	Asigna a cada caracter√≠stica (atributo o regi√≥n) una importancia para la predicci√≥n, explicando cu√°nto influy√≥ en comparaci√≥n a la media. √ötil para identificar los atributos m√°s determinantes en cada caso.	Ambos (imagen/atributos)
Concept Bottleneck	(Arquitectura interpretable) El modelo predice primero atributos y luego la clase; la explicaci√≥n es inmediata: la lista de atributos predichos act√∫a como justificaci√≥n de la clase. Permite intervenci√≥n humana sobre atributos.	Simb√≥lico (atributos)
Cada m√©todo tiene su rol: Grad-CAM y LIME ofrecen transparencia visual, mientras que las reglas, SHAP o el uso de conceptos dan transparencia l√≥gica/simb√≥lica. Idealmente, combinar√≠amos ambos tipos para una explicaci√≥n completa. Por ejemplo, para una predicci√≥n podr√≠amos mostrar un mapa Grad-CAM indicando d√≥nde mir√≥ el modelo en la foto del p√°jaro, junto con una frase basada en atributos explicando qu√© caracter√≠sticas not√≥ (y quiz√°s una regla simplificada). De esta forma, usuarios tanto t√©cnicos como no t√©cnicos (ornit√≥logos, por ejemplo) podr√≠an validar la predicci√≥n: ver√≠an que el modelo observ√≥ las partes correctas del ave y mencion√≥ las cualidades (color, forma) esperadas para esa especie, aumentando la confianza en la decisi√≥n.
5. Herramientas y Bibliotecas para la Implementaci√≥n
Para implementar el enfoque propuesto, podemos apoyarnos en diversas bibliotecas de deep learning y XAI (eXplainable AI) disponibles:
Frameworks de Deep Learning: Tanto PyTorch como TensorFlow/Keras son adecuados para construir la arquitectura multimodal. En PyTorch, se puede definir f√°cilmente un modelo con dos ramas (usando la clase torch.nn.Module personalizada) que tome dos entradas (imagen y atributos) y las combine. PyTorch Lightning tambi√©n facilita el entrenamiento organizado. En Keras (TensorFlow), la Functional API permite combinar m√∫ltiples entradas de forma declarativa
pyimagesearch.com
pyimagesearch.com
. Por ejemplo, se definir√≠a una entrada input_image (pasada por una base CNN pre-entrenada como ResNet50, posiblemente congelando capas inicialmente) y otra input_attrs (pasada por capas Dense). Luego se usa keras.layers.Concatenate() para fusionarlas y construir el resto del modelo. Ambas frameworks soportan entrenar con m√∫ltiples inputs naturalmente.
Modelos pre-entrenados: Dado el tama√±o relativamente modesto de CUB (‚âà6k im√°genes de entrenamiento), es recomendable usar un modelo de imagen pre-entrenado en ImageNet y luego afinarlo (transfer learning). En PyTorch, torchvision.models provee muchos arquitecturas CNN pre-entrenadas (ResNet, DenseNet, etc.). Podemos cargar ResNet50, reemplazar su capa final por una identidad para obtener features, y usar esas features concatenadas con los atributos. En Keras, tf.keras.applications igualmente ofrece modelos pre-entrenados listos para usar. Esto acelerar√° la convergencia y probablemente mejore la precisi√≥n.
Procesamiento de datos: Para manejar la lectura de im√°genes y atributos, bibliotecas como pandas pueden ayudar con el archivo de atributos (cargar el CSV/TXT de atributos en dataframes, pivotearlo a formato ancho por imagen). OpenCV o PIL junto con frameworks mencionados para cargar las im√°genes. Adem√°s, se pueden usar las utilidades de dataset de PyTorch (torch.utils.data.Dataset) o TensorFlow (tf.data.Dataset) para crear loaders que entreguen tuplas (imagen, atributos, etiqueta).
Explicabilidad Visual:
Grad-CAM: Existe la librer√≠a pytorch-grad-cam (de Jacob Gil) que simplifica obtener Grad-CAM en modelos PyTorch con apenas unas l√≠neas, incluso soporta modelos con m√∫ltiples inputs (tomando gradientes respecto a la imagen). En TensorFlow/Keras, hay implementaciones oficiales y oficiosas; por ejemplo tf-explain o simplemente calcular gradientes manualmente con GradientTape. Otra opci√≥n es Captum (de Facebook) para PyTorch, que ofrece captum.attr.LayerGradCam y m√©todos para Integrated Gradients, etc.
LIME: La librer√≠a lime de Python soporta explicaci√≥n de im√°genes. S√≥lo requiere proporcionar una funci√≥n lambda que dado una imagen (o lote) devuelva la predicci√≥n del modelo; luego lime_image.LimeImageExplainer segmenta la imagen y genera la explicaci√≥n. Nos dar√° las m√°scaras de superp√≠xeles m√°s importantes que podemos sobreponer en la imagen para visualizaci√≥n.
Integrated Gradients: Integrated Gradients est√° implementado en Captum (PyTorch) y en tf-explain/Alibi (Keras). Por ejemplo, Captum tiene IntegratedGradients que se usa pasando la imagen y una imagen baseline (negra) para obtener la atribuci√≥n de cada pixel.
Herramientas de visualizaci√≥n: Matplotlib puede servir para mostrar las im√°genes con los heatmaps encima. OpenCV tambi√©n, pero matplotlib integrar√° bien en notebooks. Para Grad-CAM, se suele aplicar un colormap tipo jet semi-transparente sobre la imagen original.
Explicabilidad Simb√≥lica:
Extracci√≥n de reglas: scikit-learn proporciona implementaciones de DecisionTreeClassifier que podemos entrenar sobre los atributos. Tambi√©n librer√≠as como sklearn.tree.export_text permiten volcar la estructura del √°rbol en texto legible. Si se desea reglas m√°s compactas que un √∫nico √°rbol (p.ej., reglas de asociaci√≥n), se podr√≠a usar librer√≠as de miner√≠a de datos (como mlxtend.frequent_patterns para reglas apriori) aunque tal vez sea overkill; un √°rbol ya resume bastante.
SHAP: La librer√≠a SHAP (pip install shap) funciona con modelos tipo scikit-learn y tambi√©n tiene un DeepExplainer para redes neuronales. Para nuestra mezcla, podr√≠amos aplicar KernelExplainer tratando el modelo como caja negra (aunque es lento si la dimensi√≥n es grande) o usar DeepExplainer con la parte de atributos y parte de imagen por separado. Una idea: obtener SHAP values solo del submodelo de atributos (m√°s interpretable) aliment√°ndolo con los atributos reales, para ver su contribuci√≥n. SHAP tambi√©n tiene visualizaciones listas (bar charts, force plots).
Concept bottleneck interpretability: Si implementamos el CBM, la salida intermedia de conceptos se puede inspeccionar. Si usamos PyTorch, simplemente tomando el vector de predicciones de atributos podemos interpretarlo antes de la capa final. No requiere librer√≠a especial, m√°s all√° de formatear esa info para el usuario.
Entrenamiento y evaluaci√≥n:
Entrenamiento: Adem√°s de los frameworks base, se puede usar PyTorch Lightning para estructurar el entrenamiento (separa l√≥gica de forward, training step, etc., √∫til para multimodal), o TensorFlow ModelCheckpoint y EarlyStopping callbacks para guardar el mejor modelo. Dado que es multimodal, conviene monitorear la loss total y accuracy en validaci√≥n.
Hiperpar√°metros: Probablemente necesaria cierta experimentaci√≥n con la proporci√≥n de neuronas en la rama de atributos vs visual. Una herramienta como Weights & Biases o TensorBoard puede ayudar a hacer tracking de experimentos.
Computaci√≥n: Como son ~12k im√°genes, entrenar en GPU es recomendable. La arquitectura propuesta no es excesivamente grande (ResNet50 + algunas capas densas), por lo que una sola GPU moderna podr√≠a entrenar en minutos-horas.
Evaluaci√≥n de Explicaciones:
Para visualizar explicaciones de forma interactiva, herramientas como Plotly Dash o paneles tipo TensorBoard plugins podr√≠an ser √∫tiles. Sin embargo, en un entorno de investigaci√≥n, probablemente generemos las explicaciones offline y las analicemos manualmente.
Si se quisiera integrar en una aplicaci√≥n, se podr√≠a usar Gradio o Streamlit para hacer una demo web donde un usuario sube una foto de ave y el sistema muestra la predicci√≥n con un Grad-CAM overlay y algunas frases explicativas.
En resumen, el ecosistema Python ofrece todo lo necesario: PyTorch/TensorFlow para la red multimodal, pandas/sklearn para manejar atributos y explicaciones simb√≥licas, y librer√≠as especializadas como lime, shap, captum para las explicaciones avanzadas. La recomendaci√≥n es apoyarse en estas bibliotecas en lugar de reinventar m√©todos, para asegurar resultados confiables. Por ejemplo, usar shap para computar importancias de atributos garantiza que estamos siguiendo un m√©todo bien establecido y no una heur√≠stica ad-hoc.
6. Evaluaci√≥n del Rendimiento y Calidad Explicativa
Finalmente, dise√±aremos c√≥mo evaluar tanto la precisi√≥n del modelo como la utilidad de sus explicaciones: Rendimiento del modelo: Dado que es un problema de clasificaci√≥n multiclase (200 especies), el principal m√©trico ser√° la exactitud (accuracy) sobre el conjunto de prueba. Conviene reportar tambi√©n la accuracy top-5, ya que en fine-grained a veces el modelo falla por confundir especies muy cercanas; una top-5 alta indicar√≠a que la verdadera especie suele estar entre las 5 con mayor puntaje. Adem√°s, podemos calcular una matriz de confusi√≥n para identificar patrones de error: ¬øconfunde consistentemente cierto gorri√≥n con otro de plumaje similar? Esto podr√≠a indicar qu√© atributos o rasgos est√° ignorando. Dado que integramos atributos, ser√≠a interesante comparar la performance con un modelo solo im√°genes (CNN pura) y solo atributos (por ejemplo, un clasificador entrenado √∫nicamente con el vector de 312 atributos). Es esperable que la combinaci√≥n supere a cada uno por separado; verificarlo cuantitativamente validar√° la utilidad de la multimodalidad. Tambi√©n podemos medir la p√©rdida de clasificaci√≥n (cross-entropy) y observar su convergencia durante el entrenamiento para asegurar que el modelo no est√° sobreajustando (idealmente la p√©rdida de validaci√≥n bajar√° similar a la de entrenamiento y se estabilizar√°). En caso de haber entrenado un modelo de cuello de botella de conceptos, evaluaremos tambi√©n la exactitud en la predicci√≥n de atributos (qu√© porcentaje de los 312 atributos predice correctamente por imagen). Si este accuracy de atributos es alto, entonces el modelo tiene una buena comprensi√≥n de los conceptos, lo que probablemente conduce a mejor clasificaci√≥n. Si es bajo en algunos atributos espec√≠ficos, podr√≠amos identificar cu√°les son dif√≠ciles de detectar visualmente (por ejemplo, tal vez "color de las patas" es dif√≠cil si en muchas fotos no se ven bien las patas). Calidad de las explicaciones: Evaluar explicaciones es m√°s subjetivo que evaluar precisi√≥n, pero hay estrategias tanto cualitativas como cuantitativas:
Evaluaci√≥n cualitativa (inspecci√≥n humana): Reunir un muestreo de im√°genes de prueba y para cada una mostrar la predicci√≥n del modelo junto con sus explicaciones (mapas Grad-CAM, atributos destacados, etc.). Estas explicaciones pueden ser revisadas por expertos en aves o los mismos desarrolladores para juzgar si son plausibles y coherentes con el conocimiento ornitol√≥gico. Por ejemplo, si el modelo dice "alas azules y pecho blanco" para identificar un Eastern Bluebird, y efectivamente esas son caracter√≠sticas distintivas de esa especie, la explicaci√≥n se considera buena. Si en cambio destaca algo irrelevante (fondo verde, rama en la que posa el ave) como raz√≥n, entonces la explicaci√≥n revela un posible problema (modelo sobreajustado al contexto). Esta revisi√≥n cualitativa ayuda a identificar fallos de razonamiento del modelo que no son obvios solo mirando el accuracy.
Localizaci√≥n de partes relevantes: Dado que CUB proporciona anotaciones de partes del cuerpo (coordenadas del pico, ojo, ala, cola, etc.), podemos cuantificar si los mapas de calor como Grad-CAM se solapan con las partes relevantes para la clasificaci√≥n. Por ejemplo, para cada imagen podr√≠amos comprobar si la regi√≥n de m√°xima intensidad en Grad-CAM cae dentro del cuadro delimitador del ave o de una parte particular (digamos la cabeza o alas). Una buena explicaci√≥n visual deber√≠a estar focalizada en el p√°jaro, no en el fondo. Podemos calcular la fracci√≥n de im√°genes donde el peak del mapa cae sobre el objeto correcto ("pointing game metric"). Asimismo, si sabemos que cierta parte es clave (p. ej. la mancha en la cabeza distingue dos especies), esperar√≠amos que el mapa destaque la cabeza; las anotaciones permitir√≠an verificarlo (medir IoU ‚Äì intersecci√≥n sobre uni√≥n ‚Äì entre el heatmap binarizado y la m√°scara de la cabeza, por ejemplo).
Fidelidad de las explicaciones: Para m√©todos como LIME o los √°rboles surrogate, podemos medir qu√© tan bien aproximan el modelo original. LIME ya provee una medida de ajuste local (R^2 del modelo lineal local). Para un √°rbol global, podemos computar el accuracy del √°rbol en predecir las salidas del modelo complejo en un conjunto de datos; un porcentaje alto significa que las reglas del √°rbol casi emulan al modelo. No obstante, demasiada fidelidad podr√≠a llevar a un √°rbol enorme poco interpretable, hay que balancear tama√±o vs fidelidad.
Experimentos contrafactuales: Podemos probar a modificar entradas de forma controlada para validar la explicaci√≥n. Por ejemplo, si la explicaci√≥n dijo "alas azules -> Blue Jay", podemos editar esa imagen (Photoshop, manual) para cambiar el color azul a otro color y ver si el modelo deja de predecir Blue Jay. Esto obviamente es manual y dif√≠cil a gran escala, pero se puede simular con otras im√°genes: tomar una imagen de Blue Jay y otra de un ave similar con alas diferentes, e intercambiar atributos o regiones, comprobando si el modelo cambia su predicci√≥n acorde. Si el modelo es interpretable y correcto, deber√≠a comportarse de forma consistente con los atributos (ej., si alimentamos los atributos de un Cardinal junto con la foto de un Blue Jay, ¬øqu√© hace? Idealmente deber√≠a dudar o predecir otra cosa).
M√©tricas de coherencia global: Si disponemos de muchas explicaciones, podr√≠amos evaluar si siguen un patr√≥n l√≥gico. Por ejemplo, para todas las predicciones de Blue Jay en el test, ¬øcu√°ntas veces las explicaciones mencionan "blue" o resaltan la zona azul? Deber√≠a ser frecuente. Si encontramos explicaciones dispares para la misma clase (en un caso dice alas azules, en otro dice cola larga, etc.), podr√≠a ser que el modelo use m√∫ltiples v√≠as, lo cual puede ser v√°lido si la especie tiene varios rasgos, pero tambi√©n podr√≠a indicar inconsistencia. Una medida llamada consistencia de explicaciones busca que casos similares tengan explicaciones similares. Herramientas como TCAV (Testing with Concept Activation Vectors) pueden evaluar si la sensibilidad a un concepto (p. ej. "azul") es alta para la clase que deber√≠a ser (Blue Jay) y no para otras.
Satisfacci√≥n del usuario: Si este sistema fuera para uso de expertos o ciudadanos cient√≠ficos, una evaluaci√≥n importante es la satisfacci√≥n y confianza del usuario en las explicaciones. Esto se suele medir mediante encuestas o estudios de usuario: mostrar predicciones explicadas vs no explicadas y preguntar cu√°nta confianza les genera, o si pueden detectar errores deliberados. Dado que nuestro contexto es m√°s t√©cnico, nos centramos en m√©tricas autom√°ticas, pero al final si el objetivo es "explicaciones comprensibles", la prueba de fuego es que un humano las entienda y las considere justificadas.
En conclusi√≥n, para la precisi√≥n seguiremos las m√©tricas est√°ndar de clasificaci√≥n (accuracy global y por clase, etc.), asegurando que el modelo multimodal supera baselines. Para la explicabilidad, aplicaremos una bater√≠a de m√©todos (Grad-CAM, LIME, SHAP, reglas) y evaluaremos su coherencia con el dominio del problema. Un modelo explicable ideal permitir√° afirmar con confianza: "el clasificador distingue correctamente las especies usando los mismos rasgos que usar√≠a un ornit√≥logo humano" ‚Äì y nuestras evaluaciones visuales/simb√≥licas deben corroborar esto. Si encontramos discrepancias (por ejemplo, el modelo se fija en el fondo o en un atributo irrelevante), tendremos oportunidad de depurar el modelo (ajustar el entrenamiento, agregar regularizaci√≥n, o incluso incorporar esas observaciones como retroalimentaci√≥n). La capacidad de evaluar y confiar en las explicaciones es justamente lo que diferencia este enfoque de una simple caja negra: no solo importa el porcentaje de aciertos, sino verificar c√≥mo se logran esos aciertos, garantizando un sistema m√°s transparente y fiable.